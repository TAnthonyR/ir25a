{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640be7cf",
   "metadata": {},
   "source": [
    "## .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de Texto - Funciones reutilizables\n",
    "\n",
    "# === Descarga de recursos ===\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# === Librerías necesarias ===\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# === Parte 1: Tokenización ===\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for document in corpus:\n",
    "        sentences = sent_tokenize(document)\n",
    "        tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "        tokenized_corpus.append(tokenized_sentences)\n",
    "    return tokenized_corpus\n",
    "\n",
    "# === Parte 2.1: Convertir a minúsculas ===\n",
    "def lowercase_tokens(tokenized_corpus):\n",
    "    lowercase_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lowercase_document = [[word.lower() for word in sentence] for sentence in document]\n",
    "        lowercase_corpus.append(lowercase_document)\n",
    "    return lowercase_corpus\n",
    "\n",
    "# === Parte 2.2: Eliminar puntuación y símbolos no alfabéticos ===\n",
    "def remove_punctuation(tokenized_corpus):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        cleaned_document = [[word.translate(table) for word in sentence if word.isalpha()] for sentence in document]\n",
    "        cleaned_corpus.append(cleaned_document)\n",
    "    return cleaned_corpus\n",
    "\n",
    "# === Parte 3: Eliminación de stopwords ===\n",
    "def remove_stopwords(tokenized_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        filtered_document = [[word for word in sentence if word not in stop_words] for sentence in document]\n",
    "        filtered_corpus.append(filtered_document)\n",
    "    return filtered_corpus\n",
    "\n",
    "# === Parte 4.1: Stemming ===\n",
    "def stem_tokens(tokenized_corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        stemmed_document = [[stemmer.stem(word) for word in sentence] for sentence in document]\n",
    "        stemmed_corpus.append(stemmed_document)\n",
    "    return stemmed_corpus\n",
    "\n",
    "# === Parte 4.2: Lematización ===\n",
    "def lemmatize_tokens(tokenized_corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lemmatized_document = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in document]\n",
    "        lemmatized_corpus.append(lemmatized_document)\n",
    "    return lemmatized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5770ff",
   "metadata": {},
   "source": [
    "¿Cómo usarlo?\n",
    "Importa el archivo en otro notebook: (revisar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocesamiento import tokenize_corpus, lowercase_tokens, remove_punctuation, remove_stopwords, stem_tokens, lemmatize_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095257c",
   "metadata": {},
   "source": [
    "Ejemplo de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fdb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Hello world! This is a test.\", \"Natural Language Processing with NLTK.\"]\n",
    "tokens = tokenize_corpus(corpus)\n",
    "tokens = lowercase_tokens(tokens)\n",
    "tokens = remove_punctuation(tokens)\n",
    "tokens = remove_stopwords(tokens)\n",
    "stems = stem_tokens(tokens)\n",
    "lemmas = lemmatize_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ecdc61",
   "metadata": {},
   "source": [
    "## .py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesamiento.py\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios de NLTK (solo se ejecuta una vez)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1. Tokenización\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for document in corpus:\n",
    "        sentences = sent_tokenize(document)\n",
    "        tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "        tokenized_corpus.append(tokenized_sentences)\n",
    "    return tokenized_corpus\n",
    "\n",
    "# 2. Conversión a minúsculas\n",
    "def lowercase_tokens(tokenized_corpus):\n",
    "    lowercase_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lowercase_document = [[word.lower() for word in sentence] for sentence in document]\n",
    "        lowercase_corpus.append(lowercase_document)\n",
    "    return lowercase_corpus\n",
    "\n",
    "# 3. Eliminación de puntuación y símbolos no alfabéticos\n",
    "def remove_punctuation(tokenized_corpus):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        cleaned_document = [[word.translate(table) for word in sentence if word.isalpha()] for sentence in document]\n",
    "        cleaned_corpus.append(cleaned_document)\n",
    "    return cleaned_corpus\n",
    "\n",
    "# 4. Eliminación de stopwords\n",
    "def remove_stopwords(tokenized_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        filtered_document = [[word for word in sentence if word not in stop_words] for sentence in document]\n",
    "        filtered_corpus.append(filtered_document)\n",
    "    return filtered_corpus\n",
    "\n",
    "# 5. Stemming\n",
    "def stem_tokens(tokenized_corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        stemmed_document = [[stemmer.stem(word) for word in sentence] for sentence in document]\n",
    "        stemmed_corpus.append(stemmed_document)\n",
    "    return stemmed_corpus\n",
    "\n",
    "# 6. Lematización\n",
    "def lemmatize_tokens(tokenized_corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_corpus = []\n",
    "    for document in tokenized_corpus:\n",
    "        lemmatized_document = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in document]\n",
    "        lemmatized_corpus.append(lemmatized_document)\n",
    "    return lemmatized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d2eef",
   "metadata": {},
   "source": [
    "¿Cómo usarlo?\n",
    "Guarda este contenido en un archivo llamado preprocesamiento.py y luego en tu código principal usa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocesamiento import *\n",
    "\n",
    "corpus = [\"This is an example.\", \"NLP is fun!\"]\n",
    "tokens = tokenize_corpus(corpus)\n",
    "tokens = lowercase_tokens(tokens)\n",
    "tokens = remove_punctuation(tokens)\n",
    "tokens = remove_stopwords(tokens)\n",
    "stemmed = stem_tokens(tokens)\n",
    "lemmatized = lemmatize_tokens(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
