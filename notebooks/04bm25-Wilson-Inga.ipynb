{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "941741204a003f44",
      "metadata": {
        "id": "941741204a003f44"
      },
      "source": [
        "# Ejercicio 4: Modelo Probabilístico\n",
        "\n",
        "## Objetivo de la práctica\n",
        "- Comprender los componentes del modelo vectorial mediante cálculos manuales y observación directa.\n",
        "- Aplicar el modelo de espacio vectorial con TF-IDF para recuperar documentos relevantes.\n",
        "- Comparar la recuperación con BM25 frente a TF-IDF.\n",
        "- Analizar visualmente las diferencias entre los modelos.\n",
        "- Evaluar si los rankings generados son consistentes con lo que considerarías documentos relevantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bafe7a6a4ef9e5",
      "metadata": {
        "id": "93bafe7a6a4ef9e5"
      },
      "source": [
        "## Parte 0: Carga del Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ad08bb8bd43ae327",
      "metadata": {
        "id": "ad08bb8bd43ae327"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "corpus = newsgroups.data\n",
        "target = newsgroups.target\n",
        "target_names = newsgroups.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KB3fS0sWtXw6",
      "metadata": {
        "id": "KB3fS0sWtXw6"
      },
      "source": [
        "## Normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yEKjD3MytckD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEKjD3MytckD",
        "outputId": "c2c96430-e0d2-41e3-cb54-15732885e875"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # para tokenizar el texto en palabras.\n",
        "nltk.download('stopwords') # para eliminar palabras vacías como “the”, “and”, “is”, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "mQn5SCTTtlCF",
      "metadata": {
        "id": "mQn5SCTTtlCF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def normalizar(texto):\n",
        "    texto = texto.lower()  # minúsculas\n",
        "    texto = re.sub(r'\\d+', '', texto)  # eliminar números\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # eliminar puntuación\n",
        "    tokens = word_tokenize(texto)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Aplicar al corpus\n",
        "corpus_n = [normalizar(doc) for doc in corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f8c7f78934f497",
      "metadata": {
        "id": "10f8c7f78934f497"
      },
      "source": [
        "## Parte 1: Cálculo de TF, DF, IDF y TF-IDF\n",
        "\n",
        "### Actividad\n",
        "1. Utiliza el corpus cargado.\n",
        "2. Construye la matriz de términos (TF), y calcula la frecuencia de documentos (DF)\n",
        "3. Calcula TF-IDF utilizando sklearn.\n",
        "4. Visualiza los valores en un DataFrame para analizar las diferencias entre los términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2ebd9f1c1b6c787",
      "metadata": {
        "id": "d2ebd9f1c1b6c787"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJvwqBMx0-ZI",
      "metadata": {
        "id": "oJvwqBMx0-ZI"
      },
      "source": [
        "### Matriz TF, y cálculo de la frecuencia de documentos DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "x_NZ56bW1HBQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_NZ56bW1HBQ",
        "outputId": "a422a267-4e6d-4811-8174-3c793d1bc6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape de la matriz de términos: (18846, 122066)\n",
            "       ___  ____  _____  ______  _______  ________  _________  __________  \\\n",
            "0        0     0      0       0        0         0          0           0   \n",
            "1        0     0      0       0        0         0          0           0   \n",
            "2        0     0      0       0        0         0          0           0   \n",
            "3        0     0      0       0        0         0          0           0   \n",
            "4        0     0      0       0        0         0          0           0   \n",
            "...    ...   ...    ...     ...      ...       ...        ...         ...   \n",
            "18841    0     0      0       0        0         0          0           0   \n",
            "18842    0     0      0       0        0         0          0           0   \n",
            "18843    0     0      0       0        0         0          0           0   \n",
            "18844    0     0      0       0        0         0          0           0   \n",
            "18845    0     0      0       0        0         0          0           0   \n",
            "\n",
            "       ___________  ____________  ...  zyxelb  zzc  zzip  zztvtznth  \\\n",
            "0                0             0  ...       0    0     0          0   \n",
            "1                0             0  ...       0    0     0          0   \n",
            "2                0             0  ...       0    0     0          0   \n",
            "3                0             0  ...       0    0     0          0   \n",
            "4                0             0  ...       0    0     0          0   \n",
            "...            ...           ...  ...     ...  ...   ...        ...   \n",
            "18841            0             0  ...       0    0     0          0   \n",
            "18842            0             0  ...       0    0     0          0   \n",
            "18843            0             0  ...       0    0     0          0   \n",
            "18844            0             0  ...       0    0     0          0   \n",
            "18845            0             0  ...       0    0     0          0   \n",
            "\n",
            "       zzyhcnafb_o  zzzs  zzzzzz  zzzzzzt  µsec  ÿhooked  \n",
            "0                0     0       0        0     0        0  \n",
            "1                0     0       0        0     0        0  \n",
            "2                0     0       0        0     0        0  \n",
            "3                0     0       0        0     0        0  \n",
            "4                0     0       0        0     0        0  \n",
            "...            ...   ...     ...      ...   ...      ...  \n",
            "18841            0     0       0        0     0        0  \n",
            "18842            0     0       0        0     0        0  \n",
            "18843            0     0       0        0     0        0  \n",
            "18844            0     0       0        0     0        0  \n",
            "18845            0     0       0        0     0        0  \n",
            "\n",
            "[18846 rows x 122066 columns]\n"
          ]
        }
      ],
      "source": [
        "# Crear matriz de frecuencia de términos\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(corpus_n)\n",
        "print(f\"Shape de la matriz de términos: {X_counts.shape}\")\n",
        "\n",
        "# Obtener nombres de términos\n",
        "terms = count_vect.get_feature_names_out()\n",
        "\n",
        "# Convertir a DataFrame (TF)\n",
        "tf= pd.DataFrame(X_counts.toarray(), columns=terms)\n",
        "print(tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Xwd-5PHi6Oh8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwd-5PHi6Oh8",
        "outputId": "e72b2de4-0342-48b9-da00-29303cffe18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           term  df\n",
            "0           ___  67\n",
            "1          ____  48\n",
            "2         _____  42\n",
            "3        ______  26\n",
            "4       _______  35\n",
            "...         ...  ..\n",
            "122061     zzzs   1\n",
            "122062   zzzzzz   1\n",
            "122063  zzzzzzt   1\n",
            "122064     µsec   1\n",
            "122065  ÿhooked   1\n",
            "\n",
            "[122066 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Calcular DF (cuántos documentos contienen cada término)\n",
        "df_series = (X_counts > 0).sum(axis=0).A1  # A1 convierte a arreglo plano\n",
        "df= pd.DataFrame({'term': terms, 'df': df_series})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9Q0sSYFx8gi",
      "metadata": {
        "id": "Q9Q0sSYFx8gi"
      },
      "source": [
        "### Calcular TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dW-JHTWtv1RY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW-JHTWtv1RY",
        "outputId": "707fbd46-abea-459c-fe6b-5252663df6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape del TF-IDF matrix: (18846, 122066)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "corpus_vect = vectorizer.fit_transform(corpus_n)\n",
        "print(f\"Shape del TF-IDF matrix: {corpus_vect.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Wy_63iw3wdGd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy_63iw3wdGd",
        "outputId": "ee745053-6a9c-4f6d-8b90-c743a48b9de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       ___  ____  _____  ______  _______  ________  _________  __________  \\\n",
            "0      0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "1      0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "2      0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "3      0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "4      0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "...    ...   ...    ...     ...      ...       ...        ...         ...   \n",
            "18841  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "18842  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "18843  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "18844  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "18845  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
            "\n",
            "       ___________  ____________  ...  zyxelb  zzc  zzip  zztvtznth  \\\n",
            "0              0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "1              0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "2              0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "3              0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "4              0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "...            ...           ...  ...     ...  ...   ...        ...   \n",
            "18841          0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "18842          0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "18843          0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "18844          0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "18845          0.0           0.0  ...     0.0  0.0   0.0        0.0   \n",
            "\n",
            "       zzyhcnafb_o  zzzs  zzzzzz  zzzzzzt  µsec  ÿhooked  \n",
            "0              0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "1              0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "2              0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "3              0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "4              0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "...            ...   ...     ...      ...   ...      ...  \n",
            "18841          0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "18842          0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "18843          0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "18844          0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "18845          0.0   0.0     0.0      0.0   0.0      0.0  \n",
            "\n",
            "[18846 rows x 122066 columns]\n"
          ]
        }
      ],
      "source": [
        "terms = vectorizer.get_feature_names_out()\n",
        "tfidf = pd.DataFrame(corpus_vect.toarray(), columns=terms)\n",
        "print(tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WVwdbmrSyrJU",
      "metadata": {
        "id": "WVwdbmrSyrJU"
      },
      "source": [
        "### Visualizar y analizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "AfsgNsJvx_uP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfsgNsJvx_uP",
        "outputId": "e669ee61-9b0b-4e67-909e-799fc8c82cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 términos por DF:\n",
            "          term    df\n",
            "118309   would  5259\n",
            "80782      one  5109\n",
            "52704     like  4166\n",
            "26551     dont  3889\n",
            "50176     know  3784\n",
            "36854      get  3625\n",
            "3902      also  3229\n",
            "108136   think  3127\n",
            "84169   people  2941\n",
            "108733    time  2848\n"
          ]
        }
      ],
      "source": [
        "top_df = df.sort_values(by='df', ascending=False).head(10)\n",
        "print(\"Top 10 términos por DF:\")\n",
        "print(top_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "M5zbXJgPyYla",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5zbXJgPyYla",
        "outputId": "50e09192-8ccb-481e-87c9-6f50f340292b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF del documento 0:\n",
            "pens                0.548236\n",
            "jagr                0.250831\n",
            "devils              0.216622\n",
            "bit                 0.193723\n",
            "fun                 0.185855\n",
            "regular             0.179938\n",
            "season              0.173826\n",
            "nonpittsburghers    0.169476\n",
            "bashers             0.151134\n",
            "pulp                0.151134\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF del documento 0:\")\n",
        "print(tfidf.iloc[0].sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64491bce5361e8b3",
      "metadata": {
        "id": "64491bce5361e8b3"
      },
      "source": [
        "## Parte 2: Ranking de documentos usando TF-IDF\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Dada una consulta, construye el vector de consulta\n",
        "2. Calcula la similitud coseno entre la consulta y cada documento usando los vectores TF-IDF\n",
        "3. Genera un ranking de los documentos ordenados por relevancia.\n",
        "4. Muestra los resultados en una tabla."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "va6BEzxN7lPE",
      "metadata": {
        "id": "va6BEzxN7lPE"
      },
      "source": [
        "## Crear el vector consulta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "lzHrmVuj7pN9",
      "metadata": {
        "id": "lzHrmVuj7pN9"
      },
      "outputs": [],
      "source": [
        "query = \"jersey\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "rjv_3E407vLb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjv_3E407vLb",
        "outputId": "e012573c-7582-4e08-f0bb-e65f619683db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 1 stored elements and shape (1, 122066)>\n",
            "  Coords\tValues\n",
            "  (0, 47705)\t1.0\n"
          ]
        }
      ],
      "source": [
        "query_vect = vectorizer.transform([query])\n",
        "print(query_vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHCnc0oW76b9",
      "metadata": {
        "id": "fHCnc0oW76b9"
      },
      "source": [
        "### Calcular la similitud coseno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "I3izZxqt7-wW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3izZxqt7-wW",
        "outputId": "d09eea60-72b1-458c-dfbf-da27d6648178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.10988072 0.         0.         ... 0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = cosine_similarity(query_vect, corpus_vect).flatten() # dist arreglo de similitudes de la consulta y cada documento entre [0-1]\n",
        "print(dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vL0tn_t28xI0",
      "metadata": {
        "id": "vL0tn_t28xI0"
      },
      "source": [
        "### Generar el ranking y mostrar tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8C6MdN2K8vtj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C6MdN2K8vtj",
        "outputId": "62f01f7b-38e7-47e8-aeab-078e7eb8b546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Ranking                                    Documento  Similitud coseno\n",
            "0         1                      glassboro new jersey...          0.526563\n",
            "1         2  help stuck computer new jersey access ra...          0.313241\n",
            "2         3  thanks replied initial question ive away...          0.287470\n",
            "3         4  anyone know find modem comm software app...          0.262237\n",
            "4         5  cds sale jon bon jovi new jersey boomera...          0.261063\n",
            "5         6  dont forget paul ysebaert exdevil hes go...          0.257928\n",
            "6         7  boston ottawa first period boston robert...          0.230702\n",
            "7         8  mention thread selling someones wife guy...          0.208787\n",
            "8         9  went mess new jersey still waiting refun...          0.208764\n",
            "9        10  heardperhaps incorrectly lemieux noone w...          0.205492\n",
            "10       11  toyota camry deluxe sale miles power eve...          0.204255\n",
            "11       12  ones remember offhand kdka pittsburgh pe...          0.196777\n",
            "12       13  philadelphia hartford first period hartf...          0.194554\n",
            "13       14  everyone read already sent predictions p...          0.156580\n",
            "14       15  thanks people entered years team pool su...          0.151938\n",
            "15       16  looking recommendations good great alfa ...          0.138934\n",
            "16       17  honorable placing bombs passenger airlin...          0.131735\n",
            "17       18  okay heres gripeing griping whatever liv...          0.127006\n",
            "18       19  think mike foligno captain sabres got tr...          0.115034\n",
            "19       20  subject captain ever traded resigned str...          0.114969\n",
            "20       21  taking course entitled exploring science...          0.113264\n",
            "21       22  new jersey pittsburgh first period pitts...          0.112237\n",
            "22       23  thats inner calm boredom spoiled arenas ...          0.110255\n",
            "23       24  sure bashers pens fans pretty confused l...          0.109881\n",
            "24       25  multipurpose subject lets forget shea de...          0.106203\n",
            "25       26  yet bored either people know say winning...          0.102857\n",
            "26       27  heres point far many europeans nhl sick ...          0.101786\n",
            "27       28  excellent doesnt appear little weak blin...          0.100680\n",
            "28       29  hose hose dork dork really heres posted ...          0.099387\n",
            "29       30  well thanks everyone entered far least e...          0.094931\n",
            "30       31  posting friend please respond thanks hou...          0.092786\n",
            "31       32  rangers washington first period rangers ...          0.091954\n",
            "32       33  usually one two teams changes logo minor...          0.090423\n",
            "33       34  well put jason wisconsin close relatives...          0.087711\n",
            "34       35  jesus christ score pens beating shit dev...          0.074843\n",
            "35       36  available weeklybiweeklyweekend rental b...          0.072122\n",
            "36       37  nhl playoff results conference semifinal...          0.068201\n",
            "37       38  islescaps tilts crap centre year isles w...          0.065247\n",
            "38       39  islanders washington first period island...          0.063605\n",
            "39       40  nhl playoff results conference semifinal...          0.062943\n",
            "40       41  nhl results games played standings patri...          0.062152\n",
            "41       42  atf agent interviewed street stories rep...          0.061294\n",
            "42       43  patrick roy reason game lost ron hextall...          0.059490\n",
            "43       44  private citizen would feel much secure p...          0.058062\n",
            "44       45  brings issue escrow agent paid fact gove...          0.057298\n",
            "45       46  believe raid ill planned days plan conti...          0.056337\n",
            "46       47  wondering happening minnesota seen local...          0.054870\n",
            "47       48  thinking teams mvps biggest surprises bi...          0.053783\n",
            "48       49  archivename recautospart recent changes ...          0.043593\n",
            "49       50  unlikely improbable bruins stuff nightma...          0.043572\n"
          ]
        }
      ],
      "source": [
        "top_k = 50\n",
        "\n",
        "# Ranking de indices debe ser una lista de enteros\n",
        "ranking_indices = dist.argsort()[::-1] # argsort devuelve índices de menor a mayor y con [::1] invertimos ese orden\n",
        "\n",
        "# Construcción del DataFrame\n",
        "resultados = pd.DataFrame({\n",
        "    'Ranking': range(1, top_k + 1),\n",
        "    'Documento': [corpus_n[i][:40].replace('\\n', ' ') + '...' for i in ranking_indices[:top_k]],\n",
        "    'Similitud coseno': [dist[i] for i in ranking_indices[:top_k]],\n",
        "    #'Categoría': [target_names[target[i]] for i in ranking_indices[:top_k]] #\n",
        "})\n",
        "\n",
        "print(resultados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97061325508dc5f2",
      "metadata": {
        "id": "97061325508dc5f2"
      },
      "source": [
        "## Parte 3: Ranking con BM25\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Implementa un sistema de recuperación usando el modelo BM25.\n",
        "2. Usa la misma consulta del ejercicio anterior.\n",
        "3. Calcula el score BM25 para cada documento y genera un ranking.\n",
        "4. Compara manualmente con el ranking de TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb03f7d",
      "metadata": {},
      "source": [
        "### Libreria BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9aafa361",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Ranking BM25                                    Documento  Score BM25\n",
            "0              1  cut part mcjsg ajlusxcy dkmetfqxhh mgnm ...    5.316232\n",
            "1              2  readprint reason joystick stuff take tim...    5.314773\n",
            "2              3  begin outofcontrolgif mtean___rpn_qjoxqi...    5.313636\n",
            "3              4  four pseudorandom character generators b...    5.312742\n",
            "4              5  record shows iisi without cache small en...    5.311135\n",
            "5              6  try one size rather post name email ill ...    5.290252\n",
            "6              7  part xrastool cut mfprgkwaqylqjzjhwbbxbn...    5.274772\n",
            "7              8  cut part m_xnggholizcymwtsva kxfj cabc m...    5.270097\n",
            "8              9  cut part begin wnexe mko vpivatydm mdyud...    5.269647\n",
            "9             10  season ticket holder pair giants tickets...    5.257419\n",
            "10            11  sale jazz compact discs following cds sa...    5.249742\n",
            "11            12  assume jesuss plea father let cup pass m...    5.247838\n",
            "12            13  brian guess missed point scale cold hot ...    5.240614\n",
            "13            14  following jazz magazines best offer rece...    5.240564\n",
            "14            15  wpratlantadgcom bill rawlins newsgroups ...    5.239317\n",
            "15            16  last night watching rebroadcast jerry sp...    5.236104\n",
            "16            17  part mgdydydydydydydyijfijf mdydgokzznkj...    5.234357\n",
            "17            18  dont mean sound disrespectful since majo...    5.233907\n",
            "18            19  organization aiken computation lab harva...    5.233107\n",
            "19            20  reply frankdsuucp frank odwyer problem o...    5.232706\n",
            "20            21  wayqubacuk writes single angle brackets ...    5.232286\n",
            "21            22  jesus say fulfillment law unless mistake...    5.231381\n",
            "22            23  found socculturepakistan might interest ...    5.231246\n",
            "23            24  mjmhi wondering anyone would able help t...    5.229833\n",
            "24            25  discussion objective seems falling solip...    5.225366\n",
            "25            26  last week posted alltime greatest player...    5.222818\n",
            "26            27  part mgfgzgrgnbnahfnajzahfajzahkrbrgfkhf...    5.221558\n",
            "27            28  didnt mean offend anything quoting stank...    5.219235\n",
            "28            29  theyve busines years years ago closed ra...    5.214901\n",
            "29            30  australia car enthusiast australia parti...    5.214499\n",
            "30            31  australia car enthusiast australia parti...    5.214499\n",
            "31            32  count one weak faith doubt mind right mi...    5.212494\n",
            "32            33  ran speedometer tests iisi first cache e...    5.212082\n",
            "33            34  argument lukes genealogy mary weak accor...    5.210245\n",
            "34            35  well used get mad either try communicate...    5.208656\n",
            "35            36  elias davidsson writes dear pete one zio...    5.207160\n",
            "36            37  jesus isnt god jesus returns people may ...    5.206902\n",
            "37            38  scoring stats swedish nhl players april ...    5.203444\n",
            "38            39  think weak argument fact two references ...    5.203093\n",
            "39            40  atoms objective arent even real scientis...    5.200751\n",
            "40            41  sorry friends address wants faq info jjs...    5.200066\n",
            "41            42  bubblejets often splatter little bit whe...    5.198662\n",
            "42            43  judge grant immunity whatever may learne...    5.197164\n",
            "43            44  bill coleman writes responding discussio...    5.196679\n",
            "44            45  cor judged would come judgment judged lo...    5.194456\n",
            "45            46  rick think safely say robert person unde...    5.191356\n",
            "46            47  center policy research cpr subject zioni...    5.190748\n",
            "47            48  honest answer question arabs expelled ja...    5.190344\n",
            "48            49  missing context thrilling discussion jim...    5.190059\n",
            "49            50  archivename jpegfaq lastmodified april f...    5.188355\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Crear el modelo BM25\n",
        "bm25 = BM25Okapi(corpus_n)\n",
        "# Scores BM25 para la consulta\n",
        "scores_bm25 = bm25.get_scores(query)\n",
        "# Ranking de índices BM25 \n",
        "ranking_indices_bm25 = scores_bm25.argsort()[::-1]  # Ordenar de mayor a menor\n",
        "# Construcción del DataFrame BM25\n",
        "resultados_bm25 = pd.DataFrame({\n",
        "    'Ranking BM25': range(1, top_k + 1),\n",
        "    'Documento': [corpus_n[i][:40].replace('\\n', ' ') + '...' for i in ranking_indices_bm25[:top_k]],\n",
        "    'Score BM25': [scores_bm25[i] for i in ranking_indices_bm25[:top_k]]\n",
        "})\n",
        "print(resultados_bm25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c71b85e77b4b181",
      "metadata": {
        "id": "2c71b85e77b4b181"
      },
      "source": [
        "## Parte 4: Comparación visual entre TF-IDF y BM25\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Utiliza un gráfico de barras para visualizar los scores obtenidos por cada documento según TF-IDF y BM25.\n",
        "2. Compara los rankings visualmente.\n",
        "3. Identifica: ¿Qué documentos obtienen scores más altos en un modelo que en otro?\n",
        "4. Sugiere: ¿A qué se podría deber esta diferencia?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97d171655ecfb",
      "metadata": {
        "id": "b97d171655ecfb"
      },
      "source": [
        "## Parte 5: Evaluación con consulta relevante\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Elige una consulta y define qué documentos del corpus deberían considerarse relevantes.\n",
        "2. Evalúa Precision@3 o MAP para los rankings generados con TF-IDF y BM25.\n",
        "3. Responde: ¿Cuál modelo da mejores resultados respecto a tu criterio de relevancia?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
