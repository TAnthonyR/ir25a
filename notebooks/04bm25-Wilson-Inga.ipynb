{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "941741204a003f44",
      "metadata": {
        "id": "941741204a003f44"
      },
      "source": [
        "# Ejercicio 4: Modelo Probabilístico\n",
        "\n",
        "## Objetivo de la práctica\n",
        "- Comprender los componentes del modelo vectorial mediante cálculos manuales y observación directa.\n",
        "- Aplicar el modelo de espacio vectorial con TF-IDF para recuperar documentos relevantes.\n",
        "- Comparar la recuperación con BM25 frente a TF-IDF.\n",
        "- Analizar visualmente las diferencias entre los modelos.\n",
        "- Evaluar si los rankings generados son consistentes con lo que considerarías documentos relevantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bafe7a6a4ef9e5",
      "metadata": {
        "id": "93bafe7a6a4ef9e5"
      },
      "source": [
        "## Parte 0: Carga del Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ad08bb8bd43ae327",
      "metadata": {
        "id": "ad08bb8bd43ae327"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "corpus = newsgroups.data\n",
        "target = newsgroups.target\n",
        "target_names = newsgroups.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KB3fS0sWtXw6",
      "metadata": {
        "id": "KB3fS0sWtXw6"
      },
      "source": [
        "## Normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "yEKjD3MytckD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEKjD3MytckD",
        "outputId": "c2c96430-e0d2-41e3-cb54-15732885e875"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\wil_s\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # para tokenizar el texto en palabras.\n",
        "nltk.download('stopwords') # para eliminar palabras vacías como “the”, “and”, “is”, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "mQn5SCTTtlCF",
      "metadata": {
        "id": "mQn5SCTTtlCF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def normalizar(texto):\n",
        "    texto = texto.lower()  # minúsculas\n",
        "    texto = re.sub(r'\\d+', '', texto)  # eliminar números\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # eliminar puntuación\n",
        "    tokens = word_tokenize(texto)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Aplicar al corpus\n",
        "corpus_n = [normalizar(doc) for doc in corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f8c7f78934f497",
      "metadata": {
        "id": "10f8c7f78934f497"
      },
      "source": [
        "## Parte 1: Cálculo de TF, DF, IDF y TF-IDF\n",
        "\n",
        "### Actividad\n",
        "1. Utiliza el corpus cargado.\n",
        "2. Construye la matriz de términos (TF), y calcula la frecuencia de documentos (DF)\n",
        "3. Calcula TF-IDF utilizando sklearn.\n",
        "4. Visualiza los valores en un DataFrame para analizar las diferencias entre los términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d2ebd9f1c1b6c787",
      "metadata": {
        "id": "d2ebd9f1c1b6c787"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJvwqBMx0-ZI",
      "metadata": {
        "id": "oJvwqBMx0-ZI"
      },
      "source": [
        "### Matriz TF, y cálculo de la frecuencia de documentos DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "x_NZ56bW1HBQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_NZ56bW1HBQ",
        "outputId": "a422a267-4e6d-4811-8174-3c793d1bc6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape de la matriz de términos: (18846, 122066)\n",
            "       ___  ____  _____  ______  _______  ________  _________  __________  \\\n",
            "0        0     0      0       0        0         0          0           0   \n",
            "1        0     0      0       0        0         0          0           0   \n",
            "2        0     0      0       0        0         0          0           0   \n",
            "3        0     0      0       0        0         0          0           0   \n",
            "4        0     0      0       0        0         0          0           0   \n",
            "...    ...   ...    ...     ...      ...       ...        ...         ...   \n",
            "18841    0     0      0       0        0         0          0           0   \n",
            "18842    0     0      0       0        0         0          0           0   \n",
            "18843    0     0      0       0        0         0          0           0   \n",
            "18844    0     0      0       0        0         0          0           0   \n",
            "18845    0     0      0       0        0         0          0           0   \n",
            "\n",
            "       ___________  ____________  ...  zyxelb  zzc  zzip  zztvtznth  \\\n",
            "0                0             0  ...       0    0     0          0   \n",
            "1                0             0  ...       0    0     0          0   \n",
            "2                0             0  ...       0    0     0          0   \n",
            "3                0             0  ...       0    0     0          0   \n",
            "4                0             0  ...       0    0     0          0   \n",
            "...            ...           ...  ...     ...  ...   ...        ...   \n",
            "18841            0             0  ...       0    0     0          0   \n",
            "18842            0             0  ...       0    0     0          0   \n",
            "18843            0             0  ...       0    0     0          0   \n",
            "18844            0             0  ...       0    0     0          0   \n",
            "18845            0             0  ...       0    0     0          0   \n",
            "\n",
            "       zzyhcnafb_o  zzzs  zzzzzz  zzzzzzt  µsec  ÿhooked  \n",
            "0                0     0       0        0     0        0  \n",
            "1                0     0       0        0     0        0  \n",
            "2                0     0       0        0     0        0  \n",
            "3                0     0       0        0     0        0  \n",
            "4                0     0       0        0     0        0  \n",
            "...            ...   ...     ...      ...   ...      ...  \n",
            "18841            0     0       0        0     0        0  \n",
            "18842            0     0       0        0     0        0  \n",
            "18843            0     0       0        0     0        0  \n",
            "18844            0     0       0        0     0        0  \n",
            "18845            0     0       0        0     0        0  \n",
            "\n",
            "[18846 rows x 122066 columns]\n"
          ]
        }
      ],
      "source": [
        "# Crear matriz de frecuencia de términos\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(corpus_n)\n",
        "print(f\"Shape de la matriz de términos: {X_counts.shape}\")\n",
        "\n",
        "# Obtener nombres de términos\n",
        "terms = count_vect.get_feature_names_out()\n",
        "\n",
        "# Convertir a DataFrame (TF)\n",
        "tf= pd.DataFrame(X_counts.toarray(), columns=terms)\n",
        "print(tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Xwd-5PHi6Oh8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwd-5PHi6Oh8",
        "outputId": "e72b2de4-0342-48b9-da00-29303cffe18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           term  df\n",
            "0           ___  67\n",
            "1          ____  48\n",
            "2         _____  42\n",
            "3        ______  26\n",
            "4       _______  35\n",
            "...         ...  ..\n",
            "122061     zzzs   1\n",
            "122062   zzzzzz   1\n",
            "122063  zzzzzzt   1\n",
            "122064     µsec   1\n",
            "122065  ÿhooked   1\n",
            "\n",
            "[122066 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Calcular DF (cuántos documentos contienen cada término)\n",
        "df_series = (X_counts > 0).sum(axis=0).A1  # A1 convierte a arreglo plano\n",
        "df= pd.DataFrame({'term': terms, 'df': df_series})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9Q0sSYFx8gi",
      "metadata": {
        "id": "Q9Q0sSYFx8gi"
      },
      "source": [
        "### Calcular TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dW-JHTWtv1RY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW-JHTWtv1RY",
        "outputId": "707fbd46-abea-459c-fe6b-5252663df6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape del TF-IDF matrix: (18846, 122066)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "corpus_vect = vectorizer.fit_transform(corpus_n)\n",
        "print(f\"Shape del TF-IDF matrix: {corpus_vect.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Wy_63iw3wdGd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy_63iw3wdGd",
        "outputId": "ee745053-6a9c-4f6d-8b90-c743a48b9de1"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 17.1 GiB for an array with shape (18846, 122066) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m terms = vectorizer.get_feature_names_out()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tfidf = pd.DataFrame(\u001b[43mcorpus_vect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns=terms)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(tfidf)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wil_s\\Documentos\\GitHub\\2025A\\ir25a\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1167\u001b[39m, in \u001b[36m_cs_matrix.toarray\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     order = \u001b[38;5;28mself\u001b[39m._swap(\u001b[33m'\u001b[39m\u001b[33mcf\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out.flags.c_contiguous \u001b[38;5;129;01mor\u001b[39;00m out.flags.f_contiguous):\n\u001b[32m   1169\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mOutput array must be C or F contiguous\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wil_s\\Documentos\\GitHub\\2025A\\ir25a\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1354\u001b[39m, in \u001b[36m_spbase._process_toarray_args\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1352\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 17.1 GiB for an array with shape (18846, 122066) and data type float64"
          ]
        }
      ],
      "source": [
        "terms = vectorizer.get_feature_names_out()\n",
        "tfidf = pd.DataFrame(corpus_vect.toarray(), columns=terms)\n",
        "print(tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WVwdbmrSyrJU",
      "metadata": {
        "id": "WVwdbmrSyrJU"
      },
      "source": [
        "### Visualizar y analizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AfsgNsJvx_uP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfsgNsJvx_uP",
        "outputId": "e669ee61-9b0b-4e67-909e-799fc8c82cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 términos por DF:\n",
            "          term    df\n",
            "118309   would  5259\n",
            "80782      one  5109\n",
            "52704     like  4166\n",
            "26551     dont  3889\n",
            "50176     know  3784\n",
            "36854      get  3625\n",
            "3902      also  3229\n",
            "108136   think  3127\n",
            "84169   people  2941\n",
            "108733    time  2848\n"
          ]
        }
      ],
      "source": [
        "top_df = df.sort_values(by='df', ascending=False).head(10)\n",
        "print(\"Top 10 términos por DF:\")\n",
        "print(top_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M5zbXJgPyYla",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5zbXJgPyYla",
        "outputId": "50e09192-8ccb-481e-87c9-6f50f340292b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF del documento 0:\n",
            "pens                0.548236\n",
            "jagr                0.250831\n",
            "devils              0.216622\n",
            "bit                 0.193723\n",
            "fun                 0.185855\n",
            "regular             0.179938\n",
            "season              0.173826\n",
            "nonpittsburghers    0.169476\n",
            "bashers             0.151134\n",
            "pulp                0.151134\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF del documento 0:\")\n",
        "print(tfidf.iloc[0].sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64491bce5361e8b3",
      "metadata": {
        "id": "64491bce5361e8b3"
      },
      "source": [
        "## Parte 2: Ranking de documentos usando TF-IDF\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Dada una consulta, construye el vector de consulta\n",
        "2. Calcula la similitud coseno entre la consulta y cada documento usando los vectores TF-IDF\n",
        "3. Genera un ranking de los documentos ordenados por relevancia.\n",
        "4. Muestra los resultados en una tabla."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "va6BEzxN7lPE",
      "metadata": {
        "id": "va6BEzxN7lPE"
      },
      "source": [
        "## Crear el vector consulta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "lzHrmVuj7pN9",
      "metadata": {
        "id": "lzHrmVuj7pN9"
      },
      "outputs": [],
      "source": [
        "query = \"jersey\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "rjv_3E407vLb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjv_3E407vLb",
        "outputId": "e012573c-7582-4e08-f0bb-e65f619683db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 1 stored elements and shape (1, 122066)>\n",
            "  Coords\tValues\n",
            "  (0, 47705)\t1.0\n"
          ]
        }
      ],
      "source": [
        "query_vect = vectorizer.transform([query])\n",
        "print(query_vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHCnc0oW76b9",
      "metadata": {
        "id": "fHCnc0oW76b9"
      },
      "source": [
        "### Calcular la similitud coseno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "I3izZxqt7-wW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3izZxqt7-wW",
        "outputId": "d09eea60-72b1-458c-dfbf-da27d6648178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.10988072 0.         0.         ... 0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = cosine_similarity(query_vect, corpus_vect).flatten() # dist arreglo de similitudes de la consulta y cada documento entre [0-1]\n",
        "print(dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vL0tn_t28xI0",
      "metadata": {
        "id": "vL0tn_t28xI0"
      },
      "source": [
        "### Generar el ranking y mostrar tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8C6MdN2K8vtj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C6MdN2K8vtj",
        "outputId": "62f01f7b-38e7-47e8-aeab-078e7eb8b546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Ranking                                    Documento  Similitud coseno\n",
            "0         1                      glassboro new jersey...          0.526563\n",
            "1         2  help stuck computer new jersey access ra...          0.313241\n",
            "2         3  thanks replied initial question ive away...          0.287470\n",
            "3         4  anyone know find modem comm software app...          0.262237\n",
            "4         5  cds sale jon bon jovi new jersey boomera...          0.261063\n",
            "5         6  dont forget paul ysebaert exdevil hes go...          0.257928\n",
            "6         7  boston ottawa first period boston robert...          0.230702\n",
            "7         8  mention thread selling someones wife guy...          0.208787\n",
            "8         9  went mess new jersey still waiting refun...          0.208764\n",
            "9        10  heardperhaps incorrectly lemieux noone w...          0.205492\n",
            "10       11  toyota camry deluxe sale miles power eve...          0.204255\n",
            "11       12  ones remember offhand kdka pittsburgh pe...          0.196777\n",
            "12       13  philadelphia hartford first period hartf...          0.194554\n",
            "13       14  everyone read already sent predictions p...          0.156580\n",
            "14       15  thanks people entered years team pool su...          0.151938\n",
            "15       16  looking recommendations good great alfa ...          0.138934\n",
            "16       17  honorable placing bombs passenger airlin...          0.131735\n",
            "17       18  okay heres gripeing griping whatever liv...          0.127006\n",
            "18       19  think mike foligno captain sabres got tr...          0.115034\n",
            "19       20  subject captain ever traded resigned str...          0.114969\n",
            "20       21  taking course entitled exploring science...          0.113264\n",
            "21       22  new jersey pittsburgh first period pitts...          0.112237\n",
            "22       23  thats inner calm boredom spoiled arenas ...          0.110255\n",
            "23       24  sure bashers pens fans pretty confused l...          0.109881\n",
            "24       25  multipurpose subject lets forget shea de...          0.106203\n",
            "25       26  yet bored either people know say winning...          0.102857\n",
            "26       27  heres point far many europeans nhl sick ...          0.101786\n",
            "27       28  excellent doesnt appear little weak blin...          0.100680\n",
            "28       29  hose hose dork dork really heres posted ...          0.099387\n",
            "29       30  well thanks everyone entered far least e...          0.094931\n",
            "30       31  posting friend please respond thanks hou...          0.092786\n",
            "31       32  rangers washington first period rangers ...          0.091954\n",
            "32       33  usually one two teams changes logo minor...          0.090423\n",
            "33       34  well put jason wisconsin close relatives...          0.087711\n",
            "34       35  jesus christ score pens beating shit dev...          0.074843\n",
            "35       36  available weeklybiweeklyweekend rental b...          0.072122\n",
            "36       37  nhl playoff results conference semifinal...          0.068201\n",
            "37       38  islescaps tilts crap centre year isles w...          0.065247\n",
            "38       39  islanders washington first period island...          0.063605\n",
            "39       40  nhl playoff results conference semifinal...          0.062943\n",
            "40       41  nhl results games played standings patri...          0.062152\n",
            "41       42  atf agent interviewed street stories rep...          0.061294\n",
            "42       43  patrick roy reason game lost ron hextall...          0.059490\n",
            "43       44  private citizen would feel much secure p...          0.058062\n",
            "44       45  brings issue escrow agent paid fact gove...          0.057298\n",
            "45       46  believe raid ill planned days plan conti...          0.056337\n",
            "46       47  wondering happening minnesota seen local...          0.054870\n",
            "47       48  thinking teams mvps biggest surprises bi...          0.053783\n",
            "48       49  archivename recautospart recent changes ...          0.043593\n",
            "49       50  unlikely improbable bruins stuff nightma...          0.043572\n"
          ]
        }
      ],
      "source": [
        "top_k = 50\n",
        "\n",
        "# Ranking de indices debe ser una lista de enteros\n",
        "ranking_indices = dist.argsort()[::-1] # argsort devuelve índices de menor a mayor y con [::1] invertimos ese orden\n",
        "\n",
        "# Construcción del DataFrame\n",
        "resultados = pd.DataFrame({\n",
        "    'Ranking': range(1, top_k + 1),\n",
        "    'Documento': [corpus_n[i][:40].replace('\\n', ' ') + '...' for i in ranking_indices[:top_k]],\n",
        "    'Similitud coseno': [dist[i] for i in ranking_indices[:top_k]],\n",
        "    #'Categoría': [target_names[target[i]] for i in ranking_indices[:top_k]] #\n",
        "})\n",
        "\n",
        "print(resultados)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97061325508dc5f2",
      "metadata": {
        "id": "97061325508dc5f2"
      },
      "source": [
        "## Parte 3: Ranking con BM25\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Implementa un sistema de recuperación usando el modelo BM25.\n",
        "2. Usa la misma consulta del ejercicio anterior.\n",
        "3. Calcula el score BM25 para cada documento y genera un ranking.\n",
        "4. Compara manualmente con el ranking de TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb03f7d",
      "metadata": {},
      "source": [
        "### Libreria BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9aafa361",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      Documento  Score BM25  Ranking BM25\n",
            "0   cut part mcjsg ajlusxcy dkmetfqxhh mgnm ...    5.316232             1\n",
            "1   readprint reason joystick stuff take tim...    5.314773             2\n",
            "2   begin outofcontrolgif mtean___rpn_qjoxqi...    5.313636             3\n",
            "3   four pseudorandom character generators b...    5.312742             4\n",
            "4   record shows iisi without cache small en...    5.311135             5\n",
            "5   try one size rather post name email ill ...    5.290252             6\n",
            "6   part xrastool cut mfprgkwaqylqjzjhwbbxbn...    5.274772             7\n",
            "7   cut part m_xnggholizcymwtsva kxfj cabc m...    5.270097             8\n",
            "8   cut part begin wnexe mko vpivatydm mdyud...    5.269647             9\n",
            "9   season ticket holder pair giants tickets...    5.257419            10\n",
            "10  sale jazz compact discs following cds sa...    5.249742            11\n",
            "11  assume jesuss plea father let cup pass m...    5.247838            12\n",
            "12  brian guess missed point scale cold hot ...    5.240614            13\n",
            "13  following jazz magazines best offer rece...    5.240564            14\n",
            "14  wpratlantadgcom bill rawlins newsgroups ...    5.239317            15\n",
            "15  last night watching rebroadcast jerry sp...    5.236104            16\n",
            "16  part mgdydydydydydydyijfijf mdydgokzznkj...    5.234357            17\n",
            "17  dont mean sound disrespectful since majo...    5.233907            18\n",
            "18  organization aiken computation lab harva...    5.233107            19\n",
            "19  reply frankdsuucp frank odwyer problem o...    5.232706            20\n",
            "20  wayqubacuk writes single angle brackets ...    5.232286            21\n",
            "21  jesus say fulfillment law unless mistake...    5.231381            22\n",
            "22  found socculturepakistan might interest ...    5.231246            23\n",
            "23  mjmhi wondering anyone would able help t...    5.229833            24\n",
            "24  discussion objective seems falling solip...    5.225366            25\n",
            "25  last week posted alltime greatest player...    5.222818            26\n",
            "26  part mgfgzgrgnbnahfnajzahfajzahkrbrgfkhf...    5.221558            27\n",
            "27  didnt mean offend anything quoting stank...    5.219235            28\n",
            "28  theyve busines years years ago closed ra...    5.214901            29\n",
            "29  australia car enthusiast australia parti...    5.214499            30\n",
            "30  australia car enthusiast australia parti...    5.214499            31\n",
            "31  count one weak faith doubt mind right mi...    5.212494            32\n",
            "32  ran speedometer tests iisi first cache e...    5.212082            33\n",
            "33  argument lukes genealogy mary weak accor...    5.210245            34\n",
            "34  well used get mad either try communicate...    5.208656            35\n",
            "35  elias davidsson writes dear pete one zio...    5.207160            36\n",
            "36  jesus isnt god jesus returns people may ...    5.206902            37\n",
            "37  scoring stats swedish nhl players april ...    5.203444            38\n",
            "38  think weak argument fact two references ...    5.203093            39\n",
            "39  atoms objective arent even real scientis...    5.200751            40\n",
            "40  sorry friends address wants faq info jjs...    5.200066            41\n",
            "41  bubblejets often splatter little bit whe...    5.198662            42\n",
            "42  judge grant immunity whatever may learne...    5.197164            43\n",
            "43  bill coleman writes responding discussio...    5.196679            44\n",
            "44  cor judged would come judgment judged lo...    5.194456            45\n",
            "45  rick think safely say robert person unde...    5.191356            46\n",
            "46  center policy research cpr subject zioni...    5.190748            47\n",
            "47  honest answer question arabs expelled ja...    5.190344            48\n",
            "48  missing context thrilling discussion jim...    5.190059            49\n",
            "49  archivename jpegfaq lastmodified april f...    5.188355            50\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Crear el modelo BM25\n",
        "bm25 = BM25Okapi(corpus_n)\n",
        "# Scores BM25 para la consulta\n",
        "scores_bm25 = bm25.get_scores(query)\n",
        "# Ranking de índices BM25 \n",
        "ranking_indices_bm25 = scores_bm25.argsort()[::-1]  # Ordenar de mayor a menor\n",
        "# Construcción del DataFrame BM25\n",
        "resultados_bm25 = pd.DataFrame({\n",
        "    'Documento': [corpus_n[i][:40].replace('\\n', ' ') + '...' for i in ranking_indices_bm25[:top_k]],\n",
        "    'Score BM25': [scores_bm25[i] for i in ranking_indices_bm25[:top_k]],\n",
        "    'Ranking BM25': range(1, top_k + 1)\n",
        "})\n",
        "print(resultados_bm25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c71b85e77b4b181",
      "metadata": {
        "id": "2c71b85e77b4b181"
      },
      "source": [
        "## Parte 4: Comparación visual entre TF-IDF y BM25\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Utiliza un gráfico de barras para visualizar los scores obtenidos por cada documento según TF-IDF y BM25.\n",
        "2. Compara los rankings visualmente.\n",
        "3. Identifica: ¿Qué documentos obtienen scores más altos en un modelo que en otro?\n",
        "4. Sugiere: ¿A qué se podría deber esta diferencia?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97d171655ecfb",
      "metadata": {
        "id": "b97d171655ecfb"
      },
      "source": [
        "## Parte 5: Evaluación con consulta relevante\n",
        "\n",
        "### Actividad\n",
        "\n",
        "1. Elige una consulta y define qué documentos del corpus deberían considerarse relevantes.\n",
        "2. Evalúa Precision@3 o MAP para los rankings generados con TF-IDF y BM25.\n",
        "3. Responde: ¿Cuál modelo da mejores resultados respecto a tu criterio de relevancia?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
